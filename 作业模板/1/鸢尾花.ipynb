{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "upper-accounting",
   "metadata": {},
   "source": [
    "# MindSpore入门，鸢尾花分类\n",
    "本实验将使用MindSpore深度学习框架，使用鸢尾花数据集，搭建简单的全连接神经网络，完成鸢尾花种类分类任务。\n",
    "\n",
    "鸢尾属约有300个品种，本实验将对下列3个品种进行分类：\n",
    "* setosa\n",
    "* versicolor\n",
    "* virginica\n",
    "\n",
    "数据集包含4个特征：sepal_length、sepal_width、petal_length、petal_width\n",
    "\n",
    "标签中0代表setosa、1代表versicolor、2代表virginica\n",
    "\n",
    "<img src=\"image/01.png\">\n",
    "\n",
    "MindSpore文档：https://www.mindspore.cn/doc/api_python/zh-CN/r1.1/index.html\n",
    "\n",
    "本实验需要以下第三方库：\n",
    "1. MindSpore 1.7\n",
    "2. Numpy 1.17.5\n",
    "3. Scikit-learn 0.24.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "exclusive-daniel",
   "metadata": {},
   "source": [
    "环境导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contained-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds # 载入数据集\n",
    "import mindspore.nn as nn # 网络相关\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits # 损失函数\n",
    "from mindspore.nn.metrics import Accuracy # 评估矩阵\n",
    "from mindspore import Model\n",
    "\n",
    "import os # 文件路径处理\n",
    "import numpy as np # numpy\n",
    "from sklearn.model_selection import train_test_split # 数据集划分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "electronic-conjunction",
   "metadata": {},
   "source": [
    "首先，我们用numpy读取csv格式的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "departmental-metro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "iris_data = genfromtxt('iris.csv', delimiter=',')\n",
    "print(iris_data[:10]) # 查看前10笔数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pregnant-rabbit",
   "metadata": {},
   "source": [
    "iris_data中的第1行不需要，前4列是特征，最后1列是标签。\n",
    "\n",
    "我们需要对iris_data做简单处理，并划分数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = iris_data[1:] # 移除第一行\n",
    "X = iris_data[:,:4].astype(np.float32) # 特征\n",
    "y = iris_data[:,-1].astype(np.int32) # 标签\n",
    "\n",
    "# 数据归一化\n",
    "X /= np.max(np.abs(X),axis=0)\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "functioning-ensemble",
   "metadata": {},
   "source": [
    "最后，我们将数据转换成MindSpore的Dataset格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intellectual-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_data = (X_train, y_train)\n",
    "train_data = ds.NumpySlicesDataset(train_data)\n",
    "\n",
    "# 测试集\n",
    "test_data = (X_test, y_test)\n",
    "test_data = ds.NumpySlicesDataset(test_data)\n",
    "\n",
    "# 批处理\n",
    "train_data = train_data.batch(32)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lyric-cooper",
   "metadata": {},
   "source": [
    "完成数据处理后，我们开始定义网络。\n",
    "\n",
    "这里的网络包含输入层、1个隐藏层和输出层，激活函数选择ReLU。\n",
    "\n",
    "<img src=\"image/02.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spread-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_net(nn.Cell):\n",
    "    # 定义算子\n",
    "    def __init__(self):\n",
    "        super(my_net, self).__init__()\n",
    "        self.fc1 = nn.Dense(4, 10) # 全连接层\n",
    "        self.fc2 = nn.Dense(10, 3) # 全连接层\n",
    "        self.relu = nn.ReLU() # 激活函数\n",
    "        \n",
    "    # 建构网络\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fresh-patient",
   "metadata": {},
   "source": [
    "定义好网络后，我们接着建立模型。\n",
    "\n",
    "除了网络，我们的模型还需要损失函数和优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "british-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络\n",
    "net = my_net()\n",
    "\n",
    "# 损失函数\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True) # sparse，输出不是one hot编码时设为Ture\n",
    "\n",
    "# 优化器\n",
    "lr = 0.01 # 学习率\n",
    "momentum = 0.9 # 动量\n",
    "net_opt = nn.Momentum(net.trainable_params(), lr, momentum)\n",
    "\n",
    "# 模型\n",
    "model = Model(net, net_loss, net_opt, metrics={\"accuracy\": Accuracy()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wound-audit",
   "metadata": {},
   "source": [
    "模型建立好后，就能开始训练。\n",
    "\n",
    "这里设置10次迭代。\n",
    "\n",
    "❋ 如果看到dataset sink mode的警告可以忽略，dataset sink mode将在后续介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "instrumental-thunder",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(10, train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "starting-palace",
   "metadata": {},
   "source": [
    "最后，我们用测试集评估模型的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imposed-proportion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9666666666666667}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "authorized-formation",
   "metadata": {},
   "source": [
    "## 思考\n",
    "1. 在对iris_data做简单处理时，为何要将最后一列数据改为int形态？\n",
    "    * 答：因为最后一列数据是标签，本实验所使用的损失函数（SoftmaxCrossEntropyWithLogits(sparse=True)，分类任务常用损失函数）需要int形态的标签。\n",
    "1. 什么是一个epoch？\n",
    "    * 答：一个epoch指所有的数据送入网络中完成一次前向计算及反向传播的过程。\n",
    "2. 本实验使用SoftmaxCrossEntropyWithLogits(sparse=True)损失函数，其中sparse参数为何要设置成Ture？\n",
    "    * 答：本实验使用的标签是int形态，CrossEntropy损失函数需要one_hot形式的标签，当sparse为True时MindSpore会对标签做one_hot处理。\n",
    "3. SoftmaxCrossEntropyWithLogits和SoftmaxCrossEntropy分别在什么情况下使用？\n",
    "    * 答：当输出层没有sofmax激活函数时，使用SoftmaxCrossEntropyWithLogits。若输出层有sofmax激活函数，使用SoftmaxCrossEntropy。\n",
    "4. 本实验为分类任务，如果进行回归任务，可以设置什么损失函数？\n",
    "    * 答：smoothl1loss，MSELoss（暂不支持CPU）等。\n",
    "5. 如果进行回归任务，最后的输出通道数是多少？\n",
    "    * 答：1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-parks",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
